{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trgscott/LELA70502_Coursework/blob/main/Can_language_models_generate_more_original_ideas%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Can language models generate more original ideas?**"
      ],
      "metadata": {
        "id": "9ZY92k3vvICR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code tests whether fine-tuning GPT-2 on fictional content and adapting the decoding parameters can align a model to generate more original ideas. This is tested via self-METEOR and ROUGE-L for the semantic difference between generated texts and a set of baseline texts. Answers to brainteaser puzzles are also used to test the coherence and value of generated ideas.\n",
        "\n",
        "The temperature and K values will need to be varied manually and the generation and testing codes re-run if you would like to see the differences in the parameters."
      ],
      "metadata": {
        "id": "sTg7zbDuvRgt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smrpkMspRVG8"
      },
      "source": [
        "Source for book plots:\n",
        "\n",
        "https://www.cs.cmu.edu/~dbamman/booksummaries.html\n",
        "\n",
        "Source for ratings of books:\n",
        "\n",
        "https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html\n",
        "\n",
        "Source for puzzles:\n",
        "\n",
        "https://huggingface.co/datasets/ErfanMoosaviMonazzah/brain-teasers\n",
        "\n",
        "Source for underlying training code framework:\n",
        "\n",
        "https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G1AuiGDufsl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdBR2gABxP1i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from random import randint\n",
        "import evaluate\n",
        "import nltk\n",
        "from nltk.translate import meteor\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import WordNetCorpusReader, wordnet\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLSTE6Q6YXP6"
      },
      "outputs": [],
      "source": [
        "# Set the random seed value for reproducibility\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx2F7eFmEYvq"
      },
      "source": [
        "# **Importing book plots, ratings and puzzles into pandas dataframes:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqqq8L0Ma6g"
      },
      "outputs": [],
      "source": [
        "#import plot summaries data\n",
        "!wget https://www.cs.cmu.edu/~dbamman/data/booksummaries.tar.gz\n",
        "!gunzip booksummaries.tar.gz\n",
        "!tar -xvf booksummaries.tar\n",
        "plots_df=pd.read_table(\"booksummaries/booksummaries.txt\", header=None, names=[\"Wikipedia_ID\", \"Freebase_ID\", \"title\", \"Author\", \"Publication_Date\", \"Book_Genres\", \"Plot_Summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up3czW4wEMal"
      },
      "outputs": [],
      "source": [
        "#plots_df.loc[:,[\"Plot_Summary\",\"title\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHuYJ6R7ZvGp"
      },
      "outputs": [],
      "source": [
        "#import book reviews data\n",
        "!wget https://mcauleylab.ucsd.edu/public_datasets/gdrive/goodreads/byGenre/goodreads_books_fantasy_paranormal.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1S_irgeiVn4"
      },
      "outputs": [],
      "source": [
        "reviews_df = pd.read_json('goodreads_books_fantasy_paranormal.json.gz', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwHhvGP3fjFl"
      },
      "outputs": [],
      "source": [
        "#reviews_df.loc[:,['title','average_rating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtmrI0ZbVEEu"
      },
      "outputs": [],
      "source": [
        "#match set of titles in both plot summaries and goodreads reviews data\n",
        "plots_reviews = plots_df.merge(reviews_df[['title', 'average_rating']], 'left')\n",
        "#remove any that have no ratings\n",
        "plots_reviews = plots_reviews[plots_reviews['average_rating'].notnull()]\n",
        "#deduplicate based on Wikipedia IDs\n",
        "plots_reviews = plots_reviews.drop_duplicates(subset=['Wikipedia_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8xrVAGHI2K1"
      },
      "outputs": [],
      "source": [
        "#filter to only 3.7+ rating\n",
        "plots_reviews['average_rating'] = pd.to_numeric(plots_reviews['average_rating'])\n",
        "plots_reviews = plots_reviews[plots_reviews.average_rating > 3.7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUV9cGeIImEL"
      },
      "outputs": [],
      "source": [
        "#only take the summaries\n",
        "plots_reviews = plots_reviews.Plot_Summary.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPI1GF6cnykg"
      },
      "outputs": [],
      "source": [
        "#shuffle the plot reviews\n",
        "random.shuffle(plots_reviews.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxcLb1gS4YdP"
      },
      "outputs": [],
      "source": [
        "#Convert back to series after shuffle as list, otherwise won't work for training\n",
        "plots_reviews = pd.Series(plots_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Wg24ieskDi"
      },
      "outputs": [],
      "source": [
        "#plots_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRsvE_IXpLCc"
      },
      "outputs": [],
      "source": [
        "#split the plot reviews into 90% training, 10% test\n",
        "plots_reviews_train, plots_reviews_test = train_test_split(plots_reviews, test_size=0.05, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7yrtpa5qqQF"
      },
      "outputs": [],
      "source": [
        "#print(len(plots_reviews_train))\n",
        "#print(len(plots_reviews_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezli9bxi0Jw4"
      },
      "outputs": [],
      "source": [
        "#Import the puzzles using recommended code\n",
        "splits = {'sp': 'data/sp-00000-of-00001.parquet', 'wp': 'data/wp-00000-of-00001.parquet'}\n",
        "puzzles = pd.read_parquet(\"hf://datasets/ErfanMoosaviMonazzah/brain-teasers/\" + splits[\"sp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS9py73Bq6Gk"
      },
      "outputs": [],
      "source": [
        "#shuffle the puzzles\n",
        "puzzles = puzzles.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4WVKOLWsaEx"
      },
      "outputs": [],
      "source": [
        "#puzzles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXE6E5yM0Xv8"
      },
      "outputs": [],
      "source": [
        "#Spit out the questions and answers\n",
        "puzzlesQ = puzzles.question.copy()\n",
        "puzzlesA = puzzles.answer.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvagLitgGQl"
      },
      "source": [
        "# **Testing the Wikipedia plot summaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t3W0anDw1t7"
      },
      "outputs": [],
      "source": [
        "# Self-METEOR on the baseline test set of fictional content - only needs doing once\n",
        "\n",
        "Total_METEOR = 0\n",
        "\n",
        "for i in plots_reviews_test:\n",
        "\n",
        "  METEOR = 0\n",
        "  references = [word_tokenize(txt) for txt in plots_reviews_test if txt != i] #exclude hypothesis\n",
        "  hypothesis = word_tokenize(i)\n",
        "  METEOR = nltk.translate.meteor(references,hypothesis)\n",
        "  Total_METEOR += METEOR\n",
        "\n",
        "Total_METEOR / len (plots_reviews_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emy33V868xi-"
      },
      "source": [
        "# **Testing the base model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7TASe4UiOKY"
      },
      "outputs": [],
      "source": [
        "#Load the model and tokeniser\n",
        "device=\"cuda\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgKWH3DB86qj"
      },
      "source": [
        "**Testing the base model on plot summary generations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKGoAlAzjDG-"
      },
      "outputs": [],
      "source": [
        "# encode the context/prompt that the generations will be conditioned on\n",
        "input_ids = tokenizer.encode('Here is the plot summary to a new and original science fiction novel:', return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A326XLsrikBk"
      },
      "outputs": [],
      "source": [
        "# Generating plot summaries with the base model\n",
        "predictions_vanilla=[]\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True,\n",
        "    max_length=768,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=5.0, # VARY TEMPERATURE BETWEEN 1.0-3.0-5.0\n",
        "    num_return_sequences=93 # same as the test set size\n",
        ")\n",
        "\n",
        "sample_outputs = tokenizer.batch_decode(sample_outputs, skip_special_tokens=True)\n",
        "predictions_vanilla.extend([sample.replace(\"<n>\", \"\\n\") for sample in sample_outputs])\n",
        "#for i, sample_output in enumerate(sample_outputs):\n",
        "#  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQK7tZZk2SZ9"
      },
      "outputs": [],
      "source": [
        "# Self-METEOR for the base model generations\n",
        "\n",
        "Total_METEOR = 0\n",
        "\n",
        "for i in predictions_vanilla:\n",
        "\n",
        "  METEOR = 0\n",
        "  references = [word_tokenize(txt) for txt in predictions_vanilla if txt != i] #exclude hypothesis\n",
        "  hypothesis = word_tokenize(i)\n",
        "  METEOR = nltk.translate.meteor(references,hypothesis)\n",
        "  Total_METEOR += METEOR\n",
        "\n",
        "Total_METEOR / len (predictions_vanilla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHMuWN5QGOXG"
      },
      "outputs": [],
      "source": [
        "#ROUGE-L for the base model generations vs. the baseline fictional content test set\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "referencez = []\n",
        "for i in range(93):\n",
        "  current_refs = []\n",
        "  for j in range(93):\n",
        "    current_refs.append(plots_reviews_test.iloc[j])\n",
        "  referencez.append(current_refs)\n",
        "\n",
        "predictionz = predictions_vanilla[0:93]\n",
        "resultz = rouge.compute(predictions=predictionz, references=referencez)\n",
        "print(resultz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEXKa8p79AUg"
      },
      "source": [
        "**Testing the base model on brainteaser puzzles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_6jdch51nl5"
      },
      "outputs": [],
      "source": [
        "#Generating 10 answers to the first 10 shuffled brainteaser puzzles\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "predictions_puzzles=[]\n",
        "for i in range(10):\n",
        "  input_ = tokenizer.batch_encode_plus(puzzlesQ[i:i+1], max_length=768, pad_to_max_length=True,truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "  input_ids = input_['input_ids']\n",
        "  input_mask = input_['attention_mask']\n",
        "  answers = model.generate(input_ids=input_ids.to(device),\n",
        "                         attention_mask=input_mask.to(device),\n",
        "                         no_repeat_ngram_size=2,\n",
        "                         do_sample=True,\n",
        "                         top_k=50,\n",
        "                         top_p=0.95,\n",
        "                         temperature=5.0, # VARY TEMPERATURE BETWEEN 1.0-5.0\n",
        "                         num_return_sequences=1,\n",
        "                         max_length=128,\n",
        "                          )\n",
        "  answers = tokenizer.batch_decode(answers, skip_special_tokens=True)\n",
        "  predictions_puzzles.extend([answer.replace(\"<n>\", \"\\n\") for answer in answers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0rlxOJ8zuyv"
      },
      "outputs": [],
      "source": [
        "predictions_puzzles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FSJ5e2GYMxT"
      },
      "source": [
        "# **Fine-tuning of GPT2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkQVIWT-WBUj"
      },
      "outputs": [],
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\", pad_to_max_length=True, return_tensors='pt')\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxcDhTqvyFdZ"
      },
      "outputs": [],
      "source": [
        "#use training data\n",
        "dataset = GPT2Dataset(plots_reviews_train, tokenizer, max_length=768)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoaders for dataset\n",
        "# Take data in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            dataset,\n",
        "            sampler = RandomSampler(dataset), # Select batches randomly\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XxB1DqnYgj8"
      },
      "outputs": [],
      "source": [
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "configuration.pad_token_id = tokenizer.eos_token_id\n",
        "#configuration.loss_type = ForCausalLMLoss\n",
        "\n",
        "# instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration).to(device)\n",
        "\n",
        "# this step is necessary because of the added tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm5ZL5t8ZSJp"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "epochs = 5 # change depending on how fitted to the fictional content want the fine-tuned generations to be\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVWJaThIbhq0"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBy4RsaCzLoh"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels,\n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JibiaZYedakP"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF8HH5odb7xw"
      },
      "outputs": [],
      "source": [
        "#Generating sample plot summaries from the fine-tuned model for the appendix of the report\n",
        "model.eval()\n",
        "\n",
        "prompt = \"<|startoftext|> Here is the plot summary to a new and original science fiction novel:\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                no_repeat_ngram_size=2,\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                max_length = 768,\n",
        "                                top_p=0.95,\n",
        "                                temperature=3.0,\n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmGrHCsAtIiX"
      },
      "outputs": [],
      "source": [
        "#Generating the 93 sample plot summaries with the fine-tuned model to be used for testing\n",
        "model.eval()\n",
        "\n",
        "prompt = \"<|startoftext|> Here is the plot summary to a new and original science fiction novel:\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "predictions_finetune=[]\n",
        "\n",
        "sample_outputz = model.generate(\n",
        "    generated,\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True,\n",
        "    max_length=768,\n",
        "    top_k=25, # ONCE TEMP SET TO 3.0 VARY K FROM 25-50-100\n",
        "    top_p=0.95,\n",
        "    temperature=3.0, # FIRST VARY TEMPERATURE BETWEEN 1.0-1.5-5.0, THEN SET TO 3.0 AND VARY TOP K\n",
        "    num_return_sequences=93\n",
        ")\n",
        "\n",
        "sample_outputz = tokenizer.batch_decode(sample_outputz, skip_special_tokens=True)\n",
        "predictions_finetune.extend([sample.replace(\"<n>\", \"\\n\") for sample in sample_outputz])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_86-HsjjK5x"
      },
      "outputs": [],
      "source": [
        "#Self-METEOR for the generated examples post finetune\n",
        "\n",
        "Total_METEOR = 0\n",
        "\n",
        "for i in predictions_finetune:\n",
        "\n",
        "  METEOR = 0\n",
        "  references = [word_tokenize(txt) for txt in predictions_finetune if txt != i] #exclude hypothesis\n",
        "  hypothesis = word_tokenize(i)\n",
        "  METEOR = nltk.translate.meteor(references,hypothesis)\n",
        "  Total_METEOR += METEOR\n",
        "\n",
        "Total_METEOR / len (predictions_finetune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWr5tcM-QtfW"
      },
      "outputs": [],
      "source": [
        "#ROUGE-L for the fine-tuned generations vs. the baseline fictional content test set\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "references = []\n",
        "for i in range(93):\n",
        "  current_refs = []\n",
        "  for j in range(93):\n",
        "    current_refs.append(plots_reviews_test.iloc[j])\n",
        "  references.append(current_refs)\n",
        "\n",
        "predictions = predictions_finetune[0:93]\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO3IZR1P2wqt"
      },
      "outputs": [],
      "source": [
        "#Answering brainteaser puzzles with the fine-tuned model\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "predictions_puzzlez=[]\n",
        "for i in range(10):\n",
        "  input_ = tokenizer.batch_encode_plus(puzzlesQ[i:i+1], max_length=768, pad_to_max_length=True,truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "  input_ids = input_['input_ids']\n",
        "  input_mask = input_['attention_mask']\n",
        "  answerz = model.generate(input_ids=input_ids.to(device),\n",
        "                         attention_mask=input_mask.to(device),\n",
        "                         no_repeat_ngram_size=2,\n",
        "                         do_sample=True,\n",
        "                         top_k=25, # ONCE TEMP SET TO 3.0 VARY K FROM 25-50-100\n",
        "                         top_p=0.95,\n",
        "                         temperature=3.0, # FIRST VARY TEMPERATURE BETWEEN 1.0-5.0, THEN SET TO 3.0 AND VARY TOP K\n",
        "                         num_return_sequences=1,\n",
        "                         max_length=128\n",
        "                         )\n",
        "  answerz = tokenizer.batch_decode(answerz, skip_special_tokens=True)\n",
        "  predictions_puzzlez.extend([answer.replace(\"<n>\", \"\\n\") for answer in answerz])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZcSYTibEJWD"
      },
      "outputs": [],
      "source": [
        "predictions_puzzlez"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMY99RcZnG1zrYbXRsjJn4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}